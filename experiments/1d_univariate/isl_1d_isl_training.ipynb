{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1D ISL Training Notebook\n",
        "\n",
        "This notebook trains a **1D implicit generator** using the **Invariant Statistical Loss (ISL)** on several possible 1D target distributions:\n",
        "\n",
        "- `gaussian`: $\\mathcal N(0, 1)$\n",
        "- `mixture`: symmetric 2-Gaussian mixture $0.5\\,\\mathcal N(-\\delta,1) + 0.5\\,\\mathcal N(+\\delta,1)$\n",
        "- `laplace`: Laplace$(0, b)$ (double exponential)\n",
        "- `student`: centered Student-t$(\\nu)$ (heavy tails)\n",
        "- `lognormal`: LogNormal$(0, \\sigma)$ (positive, skewed)\n",
        "- `pareto`: Pareto$(x_m, \\alpha)$ (positive heavy tail)\n",
        "- `mog3`: 3-component Gaussian mixture with default parameters\n",
        "\n",
        "The code assumes you are working in the **`isl-implicit-generative-models`** repository with a `src/isl/` package providing:\n",
        "\n",
        "- `isl.loss_1d.isl_1d_soft`\n",
        "- `isl.models.MLPGenerator`\n",
        "- `isl.utils.set_seed`, `isl.utils.get_device`, `isl.utils.ensure_dir`\n",
        "- `isl.metrics.ksd_rbf`\n",
        "\n",
        "You can adapt paths/imports if your layout differs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports and path setup\n",
        "import os\n",
        "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"  # macOS OpenMP workaround\n",
        "os.environ[\"OMP_NUM_THREADS\"] = \"1\"\n",
        "\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.distributions import Laplace, StudentT, LogNormal, Pareto\n",
        "\n",
        "# Adjust this if the notebook is not at repo root.\n",
        "ROOT = Path().resolve()\n",
        "SRC = ROOT / \"src\"\n",
        "if not SRC.exists():\n",
        "    # If the notebook lives in experiments/1d_univariate/, go two levels up\n",
        "    ROOT = Path.cwd().resolve().parents[2]\n",
        "    SRC = ROOT / \"src\"\n",
        "\n",
        "if str(SRC) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC))\n",
        "\n",
        "from isl.models import MLPGenerator\n",
        "from isl.loss_1d import isl_1d_soft\n",
        "from isl.utils import set_seed, get_device, ensure_dir\n",
        "from isl.metrics import ksd_rbf\n",
        "\n",
        "print(\"Using ROOT:\", ROOT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Target distributions: sampling functions\n",
        "\n",
        "def sample_real_gaussian(n: int, device: torch.device) -> torch.Tensor:\n",
        "    \"\"\"Sample from the 1D standard normal N(0, 1).\"\"\"\n",
        "    return torch.randn(n, device=device)\n",
        "\n",
        "\n",
        "def sample_real_mixture(n: int, device: torch.device, delta: float) -> torch.Tensor:\n",
        "    \"\"\"Sample from symmetric 2-component Gaussian mixture: 0.5 N(-delta,1) + 0.5 N(+delta,1).\"\"\"\n",
        "    signs = torch.where(\n",
        "        torch.rand(n, device=device) < 0.5,\n",
        "        torch.full((n,), -1.0, device=device),\n",
        "        torch.full((n,), +1.0, device=device),\n",
        "    )\n",
        "    eps = torch.randn(n, device=device)\n",
        "    return signs * delta + eps\n",
        "\n",
        "\n",
        "def sample_real_laplace(n: int, device: torch.device, scale: float) -> torch.Tensor:\n",
        "    \"\"\"Sample from Laplace(0, scale).\"\"\"\n",
        "    loc = torch.tensor(0.0, device=device)\n",
        "    sc = torch.tensor(scale, device=device)\n",
        "    dist = Laplace(loc=loc, scale=sc)\n",
        "    return dist.sample((n,))\n",
        "\n",
        "\n",
        "def sample_real_student(n: int, device: torch.device, df: float) -> torch.Tensor:\n",
        "    \"\"\"Sample from a centered Student-t(df).\"\"\"\n",
        "    df_t = torch.tensor(df, device=device)\n",
        "    dist = StudentT(df_t)\n",
        "    return dist.sample((n,))\n",
        "\n",
        "\n",
        "def sample_real_lognormal(n: int, device: torch.device, sigma: float) -> torch.Tensor:\n",
        "    \"\"\"Sample from LogNormal(0, sigma).\"\"\"\n",
        "    mean = torch.tensor(0.0, device=device)\n",
        "    std = torch.tensor(sigma, device=device)\n",
        "    dist = LogNormal(mean, std)\n",
        "    return dist.sample((n,))\n",
        "\n",
        "\n",
        "def sample_real_pareto(n: int, device: torch.device, xm: float, alpha: float) -> torch.Tensor:\n",
        "    \"\"\"Sample from Pareto(xm, alpha). Support: x >= xm > 0.\"\"\"\n",
        "    scale = torch.tensor(xm, device=device)\n",
        "    a = torch.tensor(alpha, device=device)\n",
        "    dist = Pareto(scale=scale, alpha=a)\n",
        "    return dist.sample((n,))\n",
        "\n",
        "\n",
        "def sample_real_mog3(\n",
        "    n: int,\n",
        "    device: torch.device,\n",
        "    means: torch.Tensor | None = None,\n",
        "    stds: torch.Tensor | None = None,\n",
        "    weights: torch.Tensor | None = None,\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"Sample from a 1D 3-component Gaussian mixture.\n",
        "\n",
        "    Default: means=[-3, 0, 3], stds=[1, 0.5, 1], weights=[0.3, 0.4, 0.3].\n",
        "    \"\"\"\n",
        "    if means is None:\n",
        "        means = torch.tensor([-3.0, 0.0, 3.0], device=device)\n",
        "    if stds is None:\n",
        "        stds = torch.tensor([1.0, 0.5, 1.0], device=device)\n",
        "    if weights is None:\n",
        "        weights = torch.tensor([0.3, 0.4, 0.3], device=device)\n",
        "\n",
        "    weights = weights / weights.sum()\n",
        "    comp_idx = torch.multinomial(weights, num_samples=n, replacement=True)\n",
        "    comp_means = means[comp_idx]\n",
        "    comp_stds = stds[comp_idx]\n",
        "\n",
        "    eps = torch.randn(n, device=device)\n",
        "    x = comp_means + comp_stds * eps\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training loop as a reusable function\n",
        "\n",
        "def train_isl_1d(\n",
        "    target: str = \"gaussian\",\n",
        "    steps: int = 5000,\n",
        "    batch_size: int = 512,\n",
        "    K: int = 32,\n",
        "    cdf_bandwidth: float = 0.15,\n",
        "    hist_sigma: float = 0.05,\n",
        "    noise_dim: int = 4,\n",
        "    hidden_dims: tuple[int, ...] = (64, 64),\n",
        "    lr: float = 1e-3,\n",
        "    mixture_delta: float = 2.0,\n",
        "    laplace_scale: float = 1.0,\n",
        "    student_df: float = 3.0,\n",
        "    lognorm_sigma: float = 0.5,\n",
        "    pareto_xm: float = 1.0,\n",
        "    pareto_alpha: float = 2.5,\n",
        "    log_every: int = 200,\n",
        "    device: torch.device | None = None,\n",
        "    outdir: Path | None = None,\n",
        "):\n",
        "    \"\"\"Train a 1D generator with ISL loss for a chosen 1D target distribution.\"\"\"\n",
        "    if device is None:\n",
        "        device = get_device(prefer_gpu=True)\n",
        "    if outdir is None:\n",
        "        outdir = ROOT / \"experiments\" / \"1d_univariate\" / \"runs\"\n",
        "    ensure_dir(outdir)\n",
        "\n",
        "    print(\"=\" * 70)\n",
        "    print(\"Train 1D generator with ISL\")\n",
        "    print(f\"  target         : {target}\")\n",
        "    print(f\"  steps          : {steps}\")\n",
        "    print(f\"  batch_size     : {batch_size}\")\n",
        "    print(f\"  K              : {K}\")\n",
        "    print(f\"  cdf_bandwidth  : {cdf_bandwidth}\")\n",
        "    print(f\"  hist_sigma     : {hist_sigma}\")\n",
        "    print(f\"  noise_dim      : {noise_dim}\")\n",
        "    print(f\"  hidden_dims    : {hidden_dims}\")\n",
        "    print(f\"  lr             : {lr}\")\n",
        "    print(f\"  mixture_delta  : {mixture_delta}\")\n",
        "    print(f\"  laplace_scale  : {laplace_scale}\")\n",
        "    print(f\"  student_df     : {student_df}\")\n",
        "    print(f\"  lognorm_sigma  : {lognorm_sigma}\")\n",
        "    print(f\"  pareto_xm      : {pareto_xm}\")\n",
        "    print(f\"  pareto_alpha   : {pareto_alpha}\")\n",
        "    print(f\"  device         : {device}\")\n",
        "    print(f\"  outdir         : {outdir}\")\n",
        "    print(\"=\" * 70)\n",
        "\n",
        "    torch.set_num_threads(1)\n",
        "\n",
        "    gen = MLPGenerator(\n",
        "        noise_dim=noise_dim,\n",
        "        data_dim=1,\n",
        "        hidden_dims=hidden_dims,\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = optim.Adam(gen.parameters(), lr=lr)\n",
        "    losses: list[float] = []\n",
        "\n",
        "    for step in range(1, steps + 1):\n",
        "        # Real samples\n",
        "        if target == \"gaussian\":\n",
        "            x_real = sample_real_gaussian(batch_size, device=device)\n",
        "        elif target == \"mixture\":\n",
        "            x_real = sample_real_mixture(batch_size, device=device, delta=mixture_delta)\n",
        "        elif target == \"laplace\":\n",
        "            x_real = sample_real_laplace(batch_size, device=device, scale=laplace_scale)\n",
        "        elif target == \"student\":\n",
        "            x_real = sample_real_student(batch_size, device=device, df=student_df)\n",
        "        elif target == \"lognormal\":\n",
        "            x_real = sample_real_lognormal(batch_size, device=device, sigma=lognorm_sigma)\n",
        "        elif target == \"pareto\":\n",
        "            x_real = sample_real_pareto(batch_size, device=device, xm=pareto_xm, alpha=pareto_alpha)\n",
        "        elif target == \"mog3\":\n",
        "            x_real = sample_real_mog3(batch_size, device=device)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown target: {target!r}\")\n",
        "\n",
        "        # Generator samples\n",
        "        z = torch.randn(batch_size, noise_dim, device=device)\n",
        "        x_fake = gen(z).view(-1)\n",
        "\n",
        "        # ISL loss\n",
        "        loss = isl_1d_soft(\n",
        "            x_real.view(-1),\n",
        "            x_fake,\n",
        "            K=K,\n",
        "            cdf_bandwidth=cdf_bandwidth,\n",
        "            hist_sigma=hist_sigma,\n",
        "            reduction=\"mean\",\n",
        "        )\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        if step % log_every == 0 or step == 1 or step == steps:\n",
        "            print(f\"[{step:5d}/{steps}] ISL loss = {loss.item():.6f}\")\n",
        "\n",
        "    # Save model\n",
        "    ckpt_path = outdir / f\"generator_1d_{target}_K{K}.pt\"\n",
        "    torch.save(gen.state_dict(), ckpt_path)\n",
        "    print(f\"\\nSaved generator checkpoint to {ckpt_path}\")\n",
        "\n",
        "    # Plot training curve\n",
        "    steps_arr = np.arange(1, len(losses) + 1)\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    plt.plot(steps_arr, losses)\n",
        "    plt.xlabel(\"Training step\")\n",
        "    plt.ylabel(\"ISL loss\")\n",
        "    plt.title(f\"1D ISL training curve ({target}, K={K})\")\n",
        "    plt.grid(True, ls=\"--\", alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    curve_path = outdir / f\"isl_1d_loss_curve_{target}_K{K}.png\"\n",
        "    plt.savefig(curve_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"Saved training curve to {curve_path}\")\n",
        "\n",
        "    # Evaluation: real vs generated\n",
        "    gen.eval()\n",
        "    with torch.no_grad():\n",
        "        N_eval = 20000\n",
        "\n",
        "        if target == \"gaussian\":\n",
        "            x_real_eval = sample_real_gaussian(N_eval, device=device)\n",
        "        elif target == \"mixture\":\n",
        "            x_real_eval = sample_real_mixture(N_eval, device=device, delta=mixture_delta)\n",
        "        elif target == \"laplace\":\n",
        "            x_real_eval = sample_real_laplace(N_eval, device=device, scale=laplace_scale)\n",
        "        elif target == \"student\":\n",
        "            x_real_eval = sample_real_student(N_eval, device=device, df=student_df)\n",
        "        elif target == \"lognormal\":\n",
        "            x_real_eval = sample_real_lognormal(N_eval, device=device, sigma=lognorm_sigma)\n",
        "        elif target == \"pareto\":\n",
        "            x_real_eval = sample_real_pareto(N_eval, device=device, xm=pareto_xm, alpha=pareto_alpha)\n",
        "        elif target == \"mog3\":\n",
        "            x_real_eval = sample_real_mog3(N_eval, device=device)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown target: {target!r}\")\n",
        "\n",
        "        z_eval = torch.randn(N_eval, noise_dim, device=device)\n",
        "        x_fake_eval = gen(z_eval).view(-1)\n",
        "\n",
        "    x_real_np = x_real_eval.cpu().numpy()\n",
        "    x_fake_np = x_fake_eval.cpu().numpy()\n",
        "\n",
        "    print(\"\\nSample statistics (generated):\")\n",
        "    print(f\"  mean ≈ {x_fake_np.mean():.4f}, std ≈ {x_fake_np.std():.4f}\")\n",
        "    print(\"  (real)    mean ≈ {:.4f}, std ≈ {:.4f}\".format(\n",
        "        x_real_np.mean(), x_real_np.std()\n",
        "    ))\n",
        "\n",
        "    # Histogram overlay\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    xmin = min(x_real_np.min(), x_fake_np.min())\n",
        "    xmax = max(x_real_np.max(), x_fake_np.max())\n",
        "    plt.hist(\n",
        "        x_real_np,\n",
        "        bins=100,\n",
        "        range=(xmin, xmax),\n",
        "        density=True,\n",
        "        alpha=0.5,\n",
        "        label=\"real\",\n",
        "    )\n",
        "    plt.hist(\n",
        "        x_fake_np,\n",
        "        bins=100,\n",
        "        range=(xmin, xmax),\n",
        "        density=True,\n",
        "        alpha=0.5,\n",
        "        label=\"generated\",\n",
        "    )\n",
        "    plt.legend()\n",
        "    plt.xlabel(\"x\")\n",
        "    plt.ylabel(\"density (hist)\")\n",
        "    plt.title(f\"Real vs generated histogram ({target}, K={K})\")\n",
        "    plt.grid(True, ls=\"--\", alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    hist_path = outdir / f\"isl_1d_hist_{target}_K{K}.png\"\n",
        "    plt.savefig(hist_path, dpi=200)\n",
        "    plt.close()\n",
        "    print(f\"Saved histogram plot to {hist_path}\")\n",
        "\n",
        "    # Optional: KSD for Gaussian\n",
        "    if target == \"gaussian\":\n",
        "        print(\"\\nComputing KSD against N(0,1)...\")\n",
        "        samples = torch.from_numpy(x_fake_np).float().unsqueeze(1).to(device)\n",
        "\n",
        "        def score_fn(x: torch.Tensor) -> torch.Tensor:\n",
        "            return -x\n",
        "\n",
        "        with torch.no_grad():\n",
        "            ksd_val = ksd_rbf(samples, score_fn).item()\n",
        "        print(f\"KSD (generated vs N(0,1)) ≈ {ksd_val:.4e}\")\n",
        "\n",
        "    return gen, np.array(losses), x_real_np, x_fake_np\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configuration cell: edit these for quick experiments\n",
        "\n",
        "seed = 42\n",
        "set_seed(seed, deterministic=False)\n",
        "\n",
        "config = {\n",
        "    \"target\": \"gaussian\",   # one of: gaussian, mixture, laplace, student, lognormal, pareto, mog3\n",
        "    \"steps\": 3000,\n",
        "    \"batch_size\": 512,\n",
        "    \"K\": 32,\n",
        "    \"noise_dim\": 4,\n",
        "    \"hidden_dims\": (64, 64),\n",
        "    \"lr\": 1e-3,\n",
        "    \"mixture_delta\": 2.0,\n",
        "    \"laplace_scale\": 1.0,\n",
        "    \"student_df\": 3.0,\n",
        "    \"lognorm_sigma\": 0.5,\n",
        "    \"pareto_xm\": 1.0,\n",
        "    \"pareto_alpha\": 2.5,\n",
        "    \"log_every\": 200,\n",
        "}\n",
        "\n",
        "device = get_device(prefer_gpu=True)\n",
        "print(\"Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run training with the above configuration\n",
        "\n",
        "gen, losses, x_real_np, x_fake_np = train_isl_1d(\n",
        "    target=config[\"target\"],\n",
        "    steps=config[\"steps\"],\n",
        "    batch_size=config[\"batch_size\"],\n",
        "    K=config[\"K\"],\n",
        "    cdf_bandwidth=0.15,\n",
        "    hist_sigma=0.05,\n",
        "    noise_dim=config[\"noise_dim\"],\n",
        "    hidden_dims=config[\"hidden_dims\"],\n",
        "    lr=config[\"lr\"],\n",
        "    mixture_delta=config[\"mixture_delta\"],\n",
        "    laplace_scale=config[\"laplace_scale\"],\n",
        "    student_df=config[\"student_df\"],\n",
        "    lognorm_sigma=config[\"lognorm_sigma\"],\n",
        "    pareto_xm=config[\"pareto_xm\"],\n",
        "    pareto_alpha=config[\"pareto_alpha\"],\n",
        "    log_every=config[\"log_every\"],\n",
        "    device=device,\n",
        ")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
